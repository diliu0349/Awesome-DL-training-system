# Awesome-DL-training-system (Target: OSDI, SOSP, NSDI, ATC, EyroSys, SIGCOMM, ASPLOS, SoCC)

#### Cluster Scheduling
[SoCC'17][Princeton] [SLAQ_Quality-Driven Scheduling for Distributed Machine Learning](https://dl.acm.org/doi/abs/10.1145/3127479.3127490)  
[EuroSys'18][HKU] [Optimus: an efficient dynamic resource scheduler for deep learning clusters](https://dl.acm.org/doi/abs/10.1145/3190508.3190517)  
[OSDI'18][MSRA] [Gandiva: Introspective Cluster Scheduling for Deep Learning](https://www.usenix.org/conference/osdi18/presentation/xiao)  
[NSDI'19][UMich] [Tiresias: A GPU Cluster Manager for Distributed Deep Learning](https://www.usenix.org/conference/nsdi19/presentation/gu)  
[NSDI'20][UWM] [Themis: Fair and Efficient GPU Cluster Scheduling](https://www.usenix.org/conference/nsdi20/presentation/mahajan)  
[OSDI'21][CMU] [Pollux: Co-adaptive Cluster Scheduling for Goodput-Optimized Deep Learning](https://www.usenix.org/conference/osdi21/presentation/qiao)  

[OSDI'20][Stanford] [Heterogeneity-Aware Cluster Scheduling Policies for Deep Learning Workloads](https://www.usenix.org/conference/osdi20/presentation/narayanan-deepak)  
[OSDI'20][Alibaba] [AntMan: Dynamic Scaling on GPU Clusters for Deep Learning](https://www.usenix.org/conference/osdi20/presentation/xiao)  
[OSDI'22][MSR] [Looking Beyond GPUs for DNN Scheduling on Multi-Tenant Clusters](https://www.usenix.org/conference/osdi22/presentation/mohan)  
[OSDI'23][IEU] [Effectively Scheduling Computational Graphs of Deep Neural Networks toward Their Domain-Specific Accelerators](https://www.usenix.org/conference/osdi23/presentation/zhao)  
[OSDI'23][NTU] [Hydro: Surrogate-Based Hyperparameter Tuning Service in Datacenters](https://www.usenix.org/conference/osdi23/presentation/hu)  



[ASPLOS'23][PKU] [ElasticFlow: An Elastic Serverless Training Platform for Distributed Deep Learning](https://dl.acm.org/doi/abs/10.1145/3575693.3575721)  
[EuroSys'23][CityU] [Lyra: Elastic Scheduling for Deep Learning Clusters](https://dl.acm.org/doi/abs/10.1145/3552326.3587445)  
[SOSP'23][CMU] [Sia: Heterogeneity-aware, goodput-optimized ML-cluster scheduling](https://dl.acm.org/doi/abs/10.1145/3600006.3613175)  



#### Framework
[OSDI'14][CMU] [Scaling Distributed Machine Learning with the Parameter Server](https://www.usenix.org/conference/osdi14/technical-sessions/presentation/li_mu)  
[OSDI'18][UCB] [Ray: A Distributed Framework for Emerging AI Applications](https://www.usenix.org/conference/osdi18/presentation/moritz)  
[OSDI'20][ByteDance] [A Unified Architecture for Accelerating Distributed DNN Training in Heterogeneous GPU/CPU Clusters](https://www.usenix.org/conference/osdi20/presentation/jiang)  
[OSDI'20][MSRA] [Retiarii: A Deep Learning Exploratory-Training Framework](https://www.usenix.org/conference/osdi20/presentation/zhang-quanlu)  
[OSDI'20][IC] [KungFu: Making Training in Distributed Machine Learning Adaptive](https://www.usenix.org/system/files/osdi20-mai.pdf)  
[OSDI'22][ZJU] [Walle: An End-to-End, General-Purpose, and Large-Scale Production System for Device-Cloud Collaborative Machine Learning](https://www.usenix.org/conference/osdi22/presentation/lv)  
[OSDI'22][Standford] [Unity: Accelerating DNN Training Through Joint Optimization of Algebraic Transformations and Parallelization](https://www.usenix.org/conference/osdi22/presentation/unger)  
[OSDI'22][UCB] [Alpa: Automating Inter- and Intra-Operator Parallelism for Distributed Deep Learning](https://www.usenix.org/conference/osdi22/presentation/zheng-lianmin)  
[OSDI'22][UCB] [AlpaServe: Statistical Multiplexing with Model Parallelism for Deep Learning Serving](https://www.usenix.org/conference/osdi23/presentation/li-zhouhan)  
[OSDI'23][UMich] [AdaEmbed: Adaptive Embedding for Large-Scale Recommendation Models](https://www.usenix.org/conference/osdi23/presentation/lai)  



#### Pipeline
[OSDI'20][JHU] [PipeSwitch: Fast Pipelined Context Switching for Deep Learning Applications](https://www.usenix.org/conference/osdi20/presentation/bai)  

#### Resource Sharing
[OSDI'20][MSRA] [HiveD: Sharing a GPU Cluster for Deep Learning with Guarantees](https://www.usenix.org/conference/osdi20/presentation/zhao-hanyu)  

#### Tensor Programs
[OSDI'20][UCB] [Ansor: Generating High-Performance Tensor Programs for Deep Learning](https://www.usenix.org/conference/osdi20/presentation/zheng)  
[OSDI'21][THU] [PET: Optimizing Tensor Programs with Partially Equivalent Transformations and Automated Corrections](https://www.usenix.org/conference/osdi21/presentation/wang)  
[OSDI'22][MSRA] [SparTA: Deep-Learning Model Sparsity via Tensor-with-Sparsity-Attribute](https://www.usenix.org/conference/osdi22/presentation/zheng-ningxin)  
[OSDI'23][THU] [EINNET: Optimizing Tensor Programs with Derivation-Based Transformations](https://www.usenix.org/conference/osdi23/presentation/zheng)  



#### Federated learning
[OSDI'21][UMich] [Oort: Efficient Federated Learning via Guided Participant Selection](https://www.usenix.org/conference/osdi21/presentation/lai)  

#### Security & Privacy
[OSDI'21][Columbia] [Privacy Budget Scheduling](https://www.usenix.org/conference/osdi21/presentation/luo)

#### Compilation
[OSDI'18][UW] [TVM: An Automated End-to-End Optimizing Compiler for Deep Learning](https://www.usenix.org/conference/osdi18/presentation/chen)  
[OSDI'20][MSRA] [Rammer: Enabling Holistic Deep Learning Compiler Optimizations with rTasks](https://www.usenix.org/conference/osdi20/presentation/ma)  
[OSDI'20][MSRA] [A Tensor Compiler for Unified Machine Learning Prediction Serving](https://www.usenix.org/conference/osdi20/presentation/nakandala)  
[OSDI'22][MSRA] [ROLLER: Fast and Efficient Tensor Compilation for Deep Learning](https://www.usenix.org/conference/osdi22/presentation/zhu)  
[OSDI'23][MSRA] [Cocktailer: Analyzing and Optimizing Dynamic Control Flow in Deep Learning](https://www.usenix.org/conference/osdi23/presentation/zhang-chen)  
[OSDI'23][MSRA] [Welder: Scheduling Deep Learning Memory Access via Tile-graph](https://www.usenix.org/conference/osdi23/presentation/shi)  

#### GNN
[OSDI'21][UCLA] [Dorylus: Affordable, Scalable, and Accurate GNN Training with Distributed CPU Servers and Serverless Threads](https://www.usenix.org/conference/osdi21/presentation/thorpe)  
[OSDI'21][UCSB] [GNNAdvisor: An Adaptive and Efficient Runtime System for GNN Acceleration on GPUs](https://www.usenix.org/conference/osdi21/presentation/wang-yuke)  
[OSDI'21][UWM] [Marius: Learning Massive Graph Embeddings on a Single Machine](https://www.usenix.org/conference/osdi21/presentation/mohoney)  
[OSDI'21][MSR] [P3: Distributed Deep Graph Learning at Scale](https://www.usenix.org/conference/osdi21/presentation/gandhi)  
[OSDI'23][UCSB] [MGG: Accelerating Graph Neural Networks with Fine-Grained Intra-Kernel Communication-Computation Pipelining on Multi-GPU Platforms](https://www.usenix.org/conference/osdi23/presentation/wang-yuke)  

